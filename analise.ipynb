{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"pablo_archs.ipynb","provenance":[],"collapsed_sections":["wlHvOPCsZ5yC","KGZswDVOaC3C","hcDGt0BnaGXY","2XgdXcA7aPfT","COixYybsaWyp","9gft-PE3aJvZ","2NF8faSog8so","5Ou-_XnZqjrY","1bJUx9Prq2xE","nk2HBr6JAMQ8","IwDVpznk2rZ0"],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a83ed4293a234eed9675674205823e25":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","state":{"_view_name":"VBoxView","_dom_classes":[],"_model_name":"VBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2f08ce94cbe0436195fd350a81f0f677","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4d018a36618f408ca4015e80fb12f086","IPY_MODEL_13db765babde4cc49bbe895cc9268d0a"]}},"2f08ce94cbe0436195fd350a81f0f677":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4d018a36618f408ca4015e80fb12f086":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","state":{"_view_name":"LabelView","style":"IPY_MODEL_c869071394b04e2ea3bbfc5f8279b144","_dom_classes":[],"description":"","_model_name":"LabelModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0.01MB of 0.01MB uploaded (0.00MB deduped)\r","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d207b594429b495eae3242d00f69215a"}},"13db765babde4cc49bbe895cc9268d0a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_30e70fc2fa6a41a39da20a55c38b8800","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ecc132591132447fb6aa2ccfdd7f6365"}},"c869071394b04e2ea3bbfc5f8279b144":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d207b594429b495eae3242d00f69215a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"30e70fc2fa6a41a39da20a55c38b8800":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ecc132591132447fb6aa2ccfdd7f6365":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"1tRBeUodgrrq"},"source":["# 1.0 Useful classes and functions"]},{"cell_type":"code","metadata":{"id":"B47RhEPVhIxP"},"source":["# import the necessary packages\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.applications import imagenet_utils\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.preprocessing.image import load_img\n","from sklearn.preprocessing import LabelEncoder\n","from imutils import paths\n","import numpy as np\n","import progressbar\n","import h5py\n","import random\n","import os\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wlHvOPCsZ5yC"},"source":["## 1.1 HDF5DatasetWriter"]},{"cell_type":"code","metadata":{"id":"k-IfBT8dhDiD"},"source":["# import the necessary packages\n","import h5py\n","import os\n","\n","class HDF5DatasetWriter:\n","  def __init__(self, dims, outputPath, dataKey=\"images\",bufSize=1000):\n","    \"\"\"\n","    The constructor to HDF5DatasetWriter accepts four parameters, two of which are optional.\n","    \n","    Args:\n","    dims: controls the dimension or shape of the data we will be storing in the dataset.\n","    if we were storing the (flattened) raw pixel intensities of the 28x28 = 784 MNIST dataset, \n","    then dims=(70000, 784).\n","    outputPath: path to where our output HDF5 file will be stored on disk.\n","    datakey: The optional dataKey is the name of the dataset that will store\n","    the data our algorithm will learn from.\n","    bufSize: controls the size of our in-memory buffer, which we default to 1,000 feature\n","    vectors/images. Once we reach bufSize, we’ll flush the buffer to the HDF5 dataset.\n","    \"\"\"\n","\n","    # check to see if the output path exists, and if so, raise\n","    # an exception\n","    if os.path.exists(outputPath):\n","      raise ValueError(\"The supplied `outputPath` already \"\n","        \"exists and cannot be overwritten. Manually delete \"\n","        \"the file before continuing.\", outputPath)\n","\n","    # open the HDF5 database for writing and create two datasets:\n","    # one to store the images/features and another to store the\n","    # class labels\n","    self.db = h5py.File(outputPath, \"w\")\n","    self.data = self.db.create_dataset(dataKey, dims,dtype=\"float\",compression='gzip')\n","    self.labels = self.db.create_dataset(\"labels\", (dims[0],),dtype=\"int\")\n","\n","    # store the buffer size, then initialize the buffer itself\n","    # along with the index into the datasets\n","    self.bufSize = bufSize\n","    self.buffer = {\"data\": [], \"labels\": []}\n","    self.idx = 0\n","\n","  def add(self, rows, labels):\n","    # add the rows and labels to the buffer\n","    self.buffer[\"data\"].extend(rows)\n","    self.buffer[\"labels\"].extend(labels)\n","\n","    # check to see if the buffer needs to be flushed to disk\n","    if len(self.buffer[\"data\"]) >= self.bufSize:\n","      self.flush()\n","\n","  def flush(self):\n","    # write the buffers to disk then reset the buffer\n","    i = self.idx + len(self.buffer[\"data\"])\n","    self.data[self.idx:i] = self.buffer[\"data\"]\n","    self.labels[self.idx:i] = self.buffer[\"labels\"]\n","    self.idx = i\n","    self.buffer = {\"data\": [], \"labels\": []}\n","\n","  def storeClassLabels(self, classLabels):\n","    # create a dataset to store the actual class label names,\n","    # then store the class labels\n","    dt = h5py.special_dtype(vlen=str) # `vlen=unicode` for Py2.7\n","    labelSet = self.db.create_dataset(\"label_names\",(len(classLabels),), dtype=dt)\n","    labelSet[:] = classLabels\n","\n","  def close(self):\n","    # check to see if there are any other entries in the buffer\n","    # that need to be flushed to disk\n","    if len(self.buffer[\"data\"]) > 0:\n","      self.flush()\n","\n","    # close the dataset\n","    self.db.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KGZswDVOaC3C"},"source":["## 1.2 Image to Array"]},{"cell_type":"code","metadata":{"id":"fgmBX1ST1F8U"},"source":["# import the necessary packages\n","from tensorflow.keras.preprocessing.image import img_to_array\n","\n","class ImageToArrayPreprocessor:\n","\tdef __init__(self, dataFormat=None):\n","\t\t# store the image data format\n","\t\tself.dataFormat = dataFormat\n","\n","\tdef preprocess(self, image):\n","\t\t# apply the Keras utility function that correctly rearranges\n","\t\t# the dimensions of the image\n","\t\treturn img_to_array(image, data_format=self.dataFormat)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hcDGt0BnaGXY"},"source":["## 1.3 AspectAware"]},{"cell_type":"code","metadata":{"id":"7Ga-FgPH1NuW"},"source":["# import the necessary packages\n","import imutils\n","import cv2\n","\n","# useful class to help the resize of images\n","class AspectAwarePreprocessor:\n","\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\n","\t\t# store the target image width, height, and interpolation\n","\t\t# method used when resizing\n","\t\tself.width = width\n","\t\tself.height = height\n","\t\tself.inter = inter\n","\n","\tdef preprocess(self, image):\n","\t\t# grab the dimensions of the image and then initialize\n","\t\t# the deltas to use when cropping\n","\t\t(h, w) = image.shape[:2]\n","\t\tdW = 0\n","\t\tdH = 0\n","\n","\t\t# if the width is smaller than the height, then resize\n","\t\t# along the width (i.e., the smaller dimension) and then\n","\t\t# update the deltas to crop the height to the desired\n","\t\t# dimension\n","\t\tif w < h:\n","\t\t\timage = imutils.resize(image, width=self.width,\n","\t\t\t\tinter=self.inter)\n","\t\t\tdH = int((image.shape[0] - self.height) / 2.0)\n","\n","\t\t# otherwise, the height is smaller than the width so\n","\t\t# resize along the height and then update the deltas\n","\t\t# crop along the width\n","\t\telse:\n","\t\t\timage = imutils.resize(image, height=self.height,\n","\t\t\t\tinter=self.inter)\n","\t\t\tdW = int((image.shape[1] - self.width) / 2.0)\n","\n","\t\t# now that our images have been resized, we need to\n","\t\t# re-grab the width and height, followed by performing\n","\t\t# the crop\n","\t\t(h, w) = image.shape[:2]\n","\t\timage = image[dH:h - dH, dW:w - dW]\n","\n","\t\t# finally, resize the image to the provided spatial\n","\t\t# dimensions to ensure our output image is always a fixed\n","\t\t# size\n","\t\treturn cv2.resize(image, (self.width, self.height),\n","\t\t\tinterpolation=self.inter)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2XgdXcA7aPfT"},"source":["## 1.4 Mean preprocessor"]},{"cell_type":"markdown","metadata":{"id":"lgq3q5uLZj-l"},"source":["Let’s get started with the mean pre-processor. We will learn how to convert an image\n","dataset to HDF5 format – part of this conversion involved computing the average Red, Green, and Blue pixel intensities across all images in the training dataset. Now that we have these averages, we are going to perform a pixel-wise subtraction of these values from our input images as a **form of data normalization**."]},{"cell_type":"code","metadata":{"id":"mfbg-dCsUEuW"},"source":["# import the necessary packages\n","import cv2\n","\n","class MeanPreprocessor:\n","\tdef __init__(self, rMean, gMean, bMean):\n","\t\t# store the Red, Green, and Blue channel averages across a\n","\t\t# training set\n","\t\tself.rMean = rMean\n","\t\tself.gMean = gMean\n","\t\tself.bMean = bMean\n","\n","\tdef preprocess(self, image):\n","\t\t# split the image into its respective Red, Green, and Blue\n","\t\t# channels\n","\t\t(B, G, R) = cv2.split(image.astype(\"float32\"))\n","\n","\t\t# subtract the means for each channel\n","\t\tR -= self.rMean\n","\t\tG -= self.gMean\n","\t\tB -= self.bMean\n","\n","        # Keep in mind that OpenCV represents images in BGR order\n","\t\t# merge the channels back together and return the image\n","\t\treturn cv2.merge([B, G, R])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"COixYybsaWyp"},"source":["## 1.5 Patch preprocessing"]},{"cell_type":"markdown","metadata":{"id":"rLxZ6UF7ahNo"},"source":["The PatchPreprocessor is responsible for randomly sampling MxN regions of an image during the training process. We apply patch preprocessing when the spatial dimensions of our input images are larger than what the CNN expects – this is a common technique to help reduce overfitting, and is, therefore, **a form of regularization**. Instead of using the entire image during training, we instead crop a random portion of it and pass it to the network."]},{"cell_type":"markdown","metadata":{"id":"hPeYByPoa6C4"},"source":["As you will see, we will construct an HDF5 dataset of Kaggle Dogs vs. Cats images where each image is 256x256 pixels. However, the AlexNet architecture that we’ll be implementing later in this lesson can only accept images of size 227x227 pixels. This is an excellent opportunity to perform data augmentation by randomly cropping a 227x227 region from the 256x256 image during training using PatchPreprocessor."]},{"cell_type":"code","metadata":{"id":"bm2bRAs2UKHj"},"source":["# import the necessary packages\n","from sklearn.feature_extraction.image import extract_patches_2d\n","\n","class PatchPreprocessor:\n","\tdef __init__(self, width, height):\n","\t\t# store the target width and height of the image\n","\t\tself.width = width\n","\t\tself.height = height\n","\n","\tdef preprocess(self, image):\n","\t\t# extract a random crop from the image with the target width\n","\t\t# and height\n","\t\treturn extract_patches_2d(image, (self.height, self.width),\n","\t\t\tmax_patches=1)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9gft-PE3aJvZ"},"source":["## 1.6 Crop preprocessor"]},{"cell_type":"markdown","metadata":{"id":"LFW6NqV2eSYY"},"source":["Next, we need to define a CropPreprocessor responsible for computing the 10-crops for oversampling. During the evaluating phase of our CNN, we’ll crop the four corners of the input image + the center region and then take their corresponding horizontal flips, for a total of ten samples per input image.\n","\n","These ten samples will be passed through the CNN, and then the probabilities averaged.\n","Applying this over-sampling method tends to include 1-2 percent increases in classification accuracy (and in some cases, even higher)."]},{"cell_type":"code","metadata":{"id":"Jc0kftNdR2lk"},"source":["# import the necessary packages\n","import numpy as np\n","import cv2\n","\n","class CropPreprocessor:\n","\tdef __init__(self, width, height, horiz=True, inter=cv2.INTER_AREA):\n","\t\t# store the target image width, height, whether or not\n","\t\t# horizontal flips should be included, along with the\n","\t\t# interpolation method used when resizing\n","\t\tself.width = width\n","\t\tself.height = height\n","\t\tself.horiz = horiz\n","\t\tself.inter = inter\n","\n","\tdef preprocess(self, image):\n","\t\t# initialize the list of crops\n","\t\tcrops = []\n","\n","\t\t# grab the width and height of the image then use these\n","\t\t# dimensions to define the corners of the image based\n","\t\t(h, w) = image.shape[:2]\n","\t\tcoords = [\n","\t\t\t[0, 0, self.width, self.height],\n","\t\t\t[w - self.width, 0, w, self.height],\n","\t\t\t[w - self.width, h - self.height, w, h],\n","\t\t\t[0, h - self.height, self.width, h]]\n","\n","\t\t# compute the center crop of the image as well\n","\t\tdW = int(0.5 * (w - self.width))\n","\t\tdH = int(0.5 * (h - self.height))\n","\t\tcoords.append([dW, dH, w - dW, h - dH])\n","\n","\t\t# loop over the coordinates, extract each of the crops,\n","\t\t# and resize each of them to a fixed size\n","\t\tfor (startX, startY, endX, endY) in coords:\n","\t\t\tcrop = image[startY:endY, startX:endX]\n","\t\t\tcrop = cv2.resize(crop, (self.width, self.height),\n","\t\t\t\tinterpolation=self.inter)\n","\t\t\tcrops.append(crop)\n","\n","\t\t# check to see if the horizontal flips should be taken\n","\t\tif self.horiz:\n","\t\t\t# compute the horizontal mirror flips for each crop\n","\t\t\tmirrors = [cv2.flip(c, 1) for c in crops]\n","\t\t\tcrops.extend(mirrors)\n","\n","\t\t# return the set of crops\n","\t\treturn np.array(crops)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2NF8faSog8so"},"source":["## 1.7 HDF5 dataset generators"]},{"cell_type":"markdown","metadata":{"id":"soLUcyH1iCOO"},"source":["Before we can implement the AlexNet architecture and train it on the Kaggle Dogs vs. Cats dataset, we first need to define a class responsible for yielding batches of images and labels from our HDF5 dataset. Section 1.1 discussed how to convert a set of images residing on disk into an HDF5 dataset – but how do we get them back out again? The answer is to define an **HDF5DatasetGenerator** class.\n","\n","Previously, all of our image datasets could be loaded into memory so we could rely on Keras generator utilities to yield our batches of images and corresponding labels. However, now that our datasets are too large to fit into memory, we need to handle implementing this generator ourselves."]},{"cell_type":"code","metadata":{"id":"uMo22QKUg7me"},"source":["# import the necessary packages\n","from tensorflow.keras.utils import to_categorical\n","import numpy as np\n","import h5py\n","\n","class HDF5DatasetGenerator:\n","    def __init__(self, dbPath, batchSize, preprocessors=None, aug=None, binarize=True, classes=2):\n","        # store the batch size, preprocessors, and data augmentor,\n","        # whether or not the labels should be binarized, along with\n","        # the total number of classes\n","        self.batchSize = batchSize\n","        self.preprocessors = preprocessors\n","        self.aug = aug\n","        self.binarize = binarize\n","        self.classes = classes\n","\n","        # open the HDF5 database for reading and determine the total\n","        # number of entries in the database\n","        self.db = h5py.File(dbPath, \"r\")\n","        self.numImages = self.db[\"labels\"].shape[0]\n","\n","    def generator(self, passes=np.inf):\n","\t\t# initialize the epoch count\n","        epochs = 0\n","\n","\t\t# keep looping infinitely -- the model will stop once we have\n","\t\t# reach the desired number of epochs\n","        while epochs < passes:\n","\t\t\t# loop over the HDF5 dataset\n","            for i in np.arange(0, self.numImages, self.batchSize):\n","\t\t\t\t# extract the images and labels from the HDF dataset\n","                images = self.db[\"images\"][i: i + self.batchSize]\n","                labels = self.db[\"labels\"][i: i + self.batchSize]\n","\n","\t\t\t\t# check to see if the labels should be binarized\n","                if self.binarize:\n","                    labels = to_categorical(labels, self.classes)\n","\n","\t\t\t\t# check to see if our preprocessors are not None\n","                if self.preprocessors is not None:\n","\t\t\t\t\t# initialize the list of processed images\n","                    procImages = []\n","\n","\t\t\t\t\t# loop over the images\n","                    for image in images:\n","\t\t\t\t\t\t# loop over the preprocessors and apply each\n","\t\t\t\t\t\t# to the image\n","                        for p in self.preprocessors:\n","                            image = p.preprocess(image)\n","\n","\t\t\t\t\t\t# update the list of processed images\n","                        procImages.append(image)\n","\n","\t\t\t\t\t# update the images array to be the processed\n","\t\t\t\t\t# images\n","                    images = np.array(procImages)\n","\n","\t\t\t\t# if the data augmenator exists, apply it\n","                if self.aug is not None:\n","                    (images, labels) = next(self.aug.flow(images,\n","                        labels, batch_size=self.batchSize))\n","\n","\t\t\t\t# yield a tuple of images and labels\n","                yield (images, labels)\n","\n","\t\t\t# increment the total number of epochs\n","            epochs += 1\n","\n","    def close(self):\n","        # close the database\n","        self.db.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Ou-_XnZqjrY"},"source":["## 1.8 Simple preprocessor"]},{"cell_type":"code","metadata":{"id":"Juf1YRHzqmeU"},"source":["# import the necessary packages\n","import cv2\n","\n","class SimplePreprocessor:\n","\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\n","\t\t# store the target image width, height, and interpolation\n","\t\t# method used when resizing\n","\t\tself.width = width\n","\t\tself.height = height\n","\t\tself.inter = inter\n","\n","\tdef preprocess(self, image):\n","\t\t# resize the image to a fixed size, ignoring the aspect\n","\t\t# ratio\n","\t\treturn cv2.resize(image, (self.width, self.height),\n","\t\t\tinterpolation=self.inter)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1bJUx9Prq2xE"},"source":["## 1.9 Training monitor"]},{"cell_type":"code","metadata":{"id":"m1JGywYeq5Wj"},"source":["# import the necessary packages\n","from tensorflow.keras.callbacks import BaseLogger\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import json\n","import os\n","\n","class TrainingMonitor(BaseLogger):\n","\tdef __init__(self, figPath, jsonPath=None, startAt=0):\n","\t\t# store the output path for the figure, the path to the JSON\n","\t\t# serialized file, and the starting epoch\n","\t\tsuper(TrainingMonitor, self).__init__()\n","\t\tself.figPath = figPath\n","\t\tself.jsonPath = jsonPath\n","\t\tself.startAt = startAt\n","\n","\tdef on_train_begin(self, logs={}):\n","\t\t# initialize the history dictionary\n","\t\tself.H = {}\n","\n","\t\t# if the JSON history path exists, load the training history\n","\t\tif self.jsonPath is not None:\n","\t\t\tif os.path.exists(self.jsonPath):\n","\t\t\t\tself.H = json.loads(open(self.jsonPath).read())\n","\n","\t\t\t\t# check to see if a starting epoch was supplied\n","\t\t\t\tif self.startAt > 0:\n","\t\t\t\t\t# loop over the entries in the history log and\n","\t\t\t\t\t# trim any entries that are past the starting\n","\t\t\t\t\t# epoch\n","\t\t\t\t\tfor k in self.H.keys():\n","\t\t\t\t\t\tself.H[k] = self.H[k][:self.startAt]\n","\n","\tdef on_epoch_end(self, epoch, logs={}):\n","\t\t# loop over the logs and update the loss, accuracy, etc.\n","\t\t# for the entire training process\n","\t\tfor (k, v) in logs.items():\n","\t\t\tl = self.H.get(k, [])\n","\t\t\tl.append(float(v))\n","\t\t\tself.H[k] = l\n","\n","\t\t# check to see if the training history should be serialized\n","\t\t# to file\n","\t\tif self.jsonPath is not None:\n","\t\t\tf = open(self.jsonPath, \"w\")\n","\t\t\tf.write(json.dumps(self.H))\n","\t\t\tf.close()\n","\n","\t\t# ensure at least two epochs have passed before plotting\n","\t\t# (epoch starts at zero)\n","\t\tif len(self.H[\"loss\"]) > 1:\n","\t\t\t# plot the training loss and accuracy\n","\t\t\tN = np.arange(0, len(self.H[\"loss\"]))\n","\t\t\tplt.style.use(\"ggplot\")\n","\t\t\tplt.figure()\n","\t\t\tplt.plot(N, self.H[\"loss\"], label=\"train_loss\")\n","\t\t\tplt.plot(N, self.H[\"val_loss\"], label=\"val_loss\")\n","\t\t\tplt.plot(N, self.H[\"accuracy\"], label=\"train_acc\")\n","\t\t\tplt.plot(N, self.H[\"val_accuracy\"], label=\"val_acc\")\n","\t\t\tplt.title(\"Training Loss and Accuracy [Epoch {}]\".format(\n","\t\t\t\tlen(self.H[\"loss\"])))\n","\t\t\tplt.xlabel(\"Epoch #\")\n","\t\t\tplt.ylabel(\"Loss/Accuracy\")\n","\t\t\tplt.legend()\n","\n","\t\t\t# save the figure\n","\t\t\tplt.savefig(self.figPath)\n","\t\t\tplt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nk2HBr6JAMQ8"},"source":["## 1.10 Rank accuracy\n"]},{"cell_type":"code","metadata":{"id":"AJCbMd1sAPP8"},"source":["# import the necessary packages\n","import numpy as np\n","\n","def rank5_accuracy(preds, labels):\n","\t# initialize the rank-1 and rank-5 accuracies\n","\trank1 = 0\n","\trank5 = 0\n","\n","\t# loop over the predictions and ground-truth labels\n","\tfor (p, gt) in zip(preds, labels):\n","\t\t# sort the probabilities by their index in descending\n","\t\t# order so that the more confident guesses are at the\n","\t\t# front of the list\n","\t\tp = np.argsort(p)[::-1]\n","\n","\t\t# check if the ground-truth label is in the top-5\n","\t\t# predictions\n","\t\tif gt in p[:5]:\n","\t\t\trank5 += 1\n","\n","\t\t# check to see if the ground-truth is the #1 prediction\n","\t\tif gt == p[0]:\n","\t\t\trank1 += 1\n","\n","\t# compute the final rank-1 and rank-5 accuracies\n","\trank1 /= float(len(preds))\n","\trank5 /= float(len(preds))\n","\n","\t# return a tuple of the rank-1 and rank-5 accuracies\n","\treturn (rank1, rank5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vrmCQKbMHI-4"},"source":["## 1.11 SimpleDatasetLoader"]},{"cell_type":"code","metadata":{"id":"wj4RbTs8HH62"},"source":["# import the necessary packages\n","import numpy as np\n","import cv2\n","import os\n","\n","# helper to load images\n","class SimpleDatasetLoader:\n","\tdef __init__(self, preprocessors=None):\n","\t\t# store the image preprocessor\n","\t\tself.preprocessors = preprocessors\n","\n","\t\t# if the preprocessors are None, initialize them as an\n","\t\t# empty list\n","\t\tif self.preprocessors is None:\n","\t\t\tself.preprocessors = []\n","\n","\tdef load(self, imagePaths, verbose=-1):\n","\t\t# initialize the list of features and labels\n","\t\tdata = []\n","\t\tlabels = []\n","\n","\t\t# loop over the input images\n","\t\tfor (i, imagePath) in enumerate(imagePaths):\n","\t\t\t# load the image and extract the class label assuming\n","\t\t\t# that our path has the following format:\n","\t\t\t# /path/to/dataset/{class}/{image}.jpg\n","\t\t\timage = cv2.imread(imagePath)\n","\t\t\tlabel = imagePath.split(os.path.sep)[-2]\n","\n","\t\t\t# check to see if our preprocessors are not None\n","\t\t\tif self.preprocessors is not None:\n","\t\t\t\t# loop over the preprocessors and apply each to\n","\t\t\t\t# the image\n","\t\t\t\tfor p in self.preprocessors:\n","\t\t\t\t\timage = p.preprocess(image)\n","\n","\t\t\t# treat our processed image as a \"feature vector\"\n","\t\t\t# by updating the data list followed by the labels\n","\t\t\tdata.append(image)\n","\t\t\tlabels.append(label)\n","\n","\t\t\t# show an update every `verbose` images\n","\t\t\tif verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n","\t\t\t\tprint(\"[INFO] processed {}/{}\".format(i + 1,\n","\t\t\t\t\tlen(imagePaths)))\n","\n","\t\t# return a tuple of the data and labels\n","\t\treturn (np.array(data), np.array(labels))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IwDVpznk2rZ0"},"source":["# 2.0 Working with HDFS and large datasets"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"esFTx_wHmni3","executionInfo":{"elapsed":3161,"status":"ok","timestamp":1619311160746,"user":{"displayName":"Pablo Emanuell","photoUrl":"","userId":"14886274924574131634"},"user_tz":180},"outputId":"5b441ac8-2f52-4080-d65d-20dfae605e53"},"source":["# download rock, paper, scissors dataset\n","!gdown --id 1ZrcUuGjgYlnu9CQnYtw6N0sshl7esfKd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1ZrcUuGjgYlnu9CQnYtw6N0sshl7esfKd\n","To: /content/rps.zip\n","237MB [00:01, 187MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-igquSvNn4G6"},"source":["!unzip rps.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LCmYyH7NoDcS"},"source":["!mkdir RPS/hdf5\n","!mkdir RPS/output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQjEjj79tCV5","executionInfo":{"elapsed":1123,"status":"ok","timestamp":1619319245670,"user":{"displayName":"Pablo Emanuell","photoUrl":"","userId":"14886274924574131634"},"user_tz":180},"outputId":"372d0c1d-0c6e-432b-dd12-574e40c7dd13"},"source":["!find RPS/train -type f | wc -l"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2520\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HmzjpYrE7umB","executionInfo":{"elapsed":1053,"status":"ok","timestamp":1619319164458,"user":{"displayName":"Pablo Emanuell","photoUrl":"","userId":"14886274924574131634"},"user_tz":180},"outputId":"697ba6cc-3cab-48a0-8db5-227b422b185a"},"source":["!find RPS/test -type f | wc -l"],"execution_count":null,"outputs":[{"output_type":"stream","text":["372\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jl7hYvPJo4y_"},"source":["# define the paths to the images directory\n","IMAGES_PATH = \"RPS\"\n","\n","# since we do not have validation data or access to the testing\n","# labels we need to take a number of images from the training\n","# data and use them instead\n","# REMOVED\n","# NUM_CLASSES = 2\n","# NUM_VAL_IMAGES = 1250 * NUM_CLASSES\n","# NUM_TEST_IMAGES = 1250 * NUM_CLASSES\n","\n","# define the path to the output training, validation, and testing\n","# HDF5 files\n","TRAIN_HDF5 = \"RPS/hdf5/train.hdf5\"\n","VAL_HDF5 = \"RPS/hdf5/val.hdf5\"\n","TEST_HDF5 = \"RPS/hdf5/test.hdf5\"\n","\n","# path to the output model file\n","MODEL_PATH = \"RPS/alexnet_dogs_vs_cats.model\"\n","\n","# define the path to the dataset mean\n","DATASET_MEAN = \"RPS/rps_mean.json\"\n","\n","# define the path to the output directory used for storing plots,\n","# classification reports, etc.\n","OUTPUT_PATH = \"RPS/output\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Omp0jxsm9Yr9"},"source":["# import the necessary packages\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from imutils import paths\n","import numpy as np\n","import progressbar\n","import json\n","import cv2\n","import os\n","\n","pattern = re.compile(r\"(?:test)?([a-z]+)\")\n","\n","# grab the paths to the images\n","trainPaths = list(paths.list_images(IMAGES_PATH+\"/train/\"))\n","testPaths = list(paths.list_images(IMAGES_PATH+\"/test/\")) \n","valPaths = list(paths.list_images(IMAGES_PATH+\"/validation/\"))\n","\n","trainLabels = [re.search(pattern, p.split(os.path.sep)[-1]).groups()[0] for p in trainPaths]\n","testLabels = [re.search(pattern, p.split(os.path.sep)[-1]).groups()[0] for p in testPaths]\n","valLabels = [re.search(pattern, p.split(os.path.sep)[-1]).groups()[0] for p in valPaths]\n","\n","le = LabelEncoder()\n","trainLabels = le.fit_transform(trainLabels)\n","testLabels = le.fit_transform(testLabels)\n","valLabels = le.fit_transform(valLabels)\n","\n","# construct a list pairing the training, validation, and testing\n","# image paths along with their corresponding labels and output HDF5\n","# files\n","datasets = [\n","\t(\"train\", trainPaths, trainLabels, TRAIN_HDF5),\n","\t(\"val\", valPaths, valLabels, VAL_HDF5),\n","\t(\"test\", testPaths, testLabels, TEST_HDF5)]\n","\n","# initialize the image pre-processor and the lists of RGB channel\n","# averages\n","aap = AspectAwarePreprocessor(256, 256)\n","(R, G, B) = ([], [], [])\n","\n","# loop over the dataset tuples\n","for (dType, paths, labels, outputPath) in datasets:\n","\t# create HDF5 writer\n","\tprint(\"\\n[INFO] building {}...\".format(outputPath))\n","\twriter = HDF5DatasetWriter((len(paths), 256, 256, 3), outputPath)\n","\n","\t# initialize the progress bar\n","\twidgets = [\"Building Dataset: \", progressbar.Percentage(), \" \",progressbar.Bar(), \" \", progressbar.ETA()]\n","\tpbar = progressbar.ProgressBar(maxval=len(paths),widgets=widgets).start()\n","\n","\t# loop over the image paths\n","\tfor (i, (path, label)) in enumerate(zip(paths, labels)):\n","\t\t# load the image and process it\n","\t\timage = cv2.imread(path)\n","\t\timage = aap.preprocess(image)\n","\n","\t\t# if we are building the training dataset, then compute the\n","\t\t# mean of each channel in the image, then update the\n","\t\t# respective lists\n","\t\tif dType == \"train\":\n","\t\t\t(b, g, r) = cv2.mean(image)[:3]\n","\t\t\tR.append(r)\n","\t\t\tG.append(g)\n","\t\t\tB.append(b)\n","\n","\t\t# add the image and label # to the HDF5 dataset\n","\t\twriter.add([image], [label])\n","\t\tpbar.update(i)\n","\n","\t# close the HDF5 writer\n","\tpbar.finish()\n","\twriter.close()\n","\n","# construct a dictionary of averages, then serialize the means to a\n","# JSON file\n","print(\"[INFO] serializing means...\")\n","D = {\"R\": np.mean(R), \"G\": np.mean(G), \"B\": np.mean(B)}\n","f = open(DATASET_MEAN, \"w\")\n","f.write(json.dumps(D))\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HkXIZpCslRci"},"source":["!du -h"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L0HOpKBS-pA7","executionInfo":{"elapsed":721,"status":"ok","timestamp":1619311293835,"user":{"displayName":"Pablo Emanuell","photoUrl":"","userId":"14886274924574131634"},"user_tz":180},"outputId":"01888853-9de4-41d5-fdc0-bfd1d4e139aa"},"source":["ls RPS"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mhdf5\u001b[0m/  \u001b[01;34moutput\u001b[0m/  rps_mean.json  \u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/  \u001b[01;34mvalidation\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XARO_wxt0ye0"},"source":["!cp -r RPS/hdf5/ /content/drive/MyDrive/data/\n","!cp -r RPS/output/ /content/drive/MyDrive/data/\n","!cp RPS/rps_mean.json /content/drive/MyDrive/data/\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RFkeXcghlU6a"},"source":["# 3.0 Deep learning: Rock, Paper and Scissors "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H6jJGsLu1RIk","executionInfo":{"status":"ok","timestamp":1619533102316,"user_tz":180,"elapsed":26829,"user":{"displayName":"Pablo Emanuell","photoUrl":"","userId":"14886274924574131634"}},"outputId":"258b3dd6-fe07-4d8a-ce01-24861750a85a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iVqQ7et85T60"},"source":["# \n","# only in case load data from drive\n","#\n","!mkdir RPS\n","!cp -r /content/drive/MyDrive/data/hdf5 RPS\n","!cp -r /content/drive/MyDrive/data/output RPS\n","!cp /content/drive/MyDrive/data/rps_mean.json RPS"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vc_G18s2S0o_"},"source":["## Setup W&B"]},{"cell_type":"code","metadata":{"id":"ASK0J-slUKYa"},"source":["%%capture\n","!pip install wandb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ZePtP8nUYmY","executionInfo":{"status":"ok","timestamp":1619478087390,"user_tz":180,"elapsed":1894,"user":{"displayName":"Pablo Emanuell","photoUrl":"","userId":"14886274924574131634"}},"outputId":"3d994bd3-2b21-4f0a-b993-18e33c971002"},"source":["!wandb login"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpabloufrn\u001b[0m (use `wandb login --relogin` to force relogin)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dIBKr3k0TPWT","colab":{"base_uri":"https://localhost:8080/","height":306,"referenced_widgets":["a83ed4293a234eed9675674205823e25","2f08ce94cbe0436195fd350a81f0f677","4d018a36618f408ca4015e80fb12f086","13db765babde4cc49bbe895cc9268d0a","c869071394b04e2ea3bbfc5f8279b144","d207b594429b495eae3242d00f69215a","30e70fc2fa6a41a39da20a55c38b8800","ecc132591132447fb6aa2ccfdd7f6365"]},"executionInfo":{"status":"ok","timestamp":1619492212685,"user_tz":180,"elapsed":5642,"user":{"displayName":"Pablo Emanuell","photoUrl":"","userId":"14886274924574131634"}},"outputId":"26920dab-eae0-4902-82ff-bd357e6a73f1"},"source":["import wandb\n","from wandb.keras import WandbCallback\n","\n","# Default values for hyperparameters\n","defaults = dict(\n","                batch_size = 128)\n","\n","wandb.init(project=\"rps_cnn_final\", config=defaults, name=\"final\")\n","config = wandb.config"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["Finishing last run (ID:3nzdwa24) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<br/>Waiting for W&B process to finish, PID 484<br/>Program ended successfully."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a83ed4293a234eed9675674205823e25","version_minor":0,"version_major":2},"text/plain":["VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["Find user logs for this run at: <code>/content/wandb/run-20210427_025608-3nzdwa24/logs/debug.log</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["Find internal logs for this run at: <code>/content/wandb/run-20210427_025608-3nzdwa24/logs/debug-internal.log</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","                    <br/>Synced <strong style=\"color:#cdcd00\">final</strong>: <a href=\"https://wandb.ai/pabloufrn/rps_cnn_final/runs/3nzdwa24\" target=\"_blank\">https://wandb.ai/pabloufrn/rps_cnn_final/runs/3nzdwa24</a><br/>\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["...Successfully finished last run (ID:3nzdwa24). Initializing new run:<br/><br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","                Tracking run with wandb version 0.10.27<br/>\n","                Syncing run <strong style=\"color:#cdcd00\">final</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n","                Project page: <a href=\"https://wandb.ai/pabloufrn/rps_cnn_final\" target=\"_blank\">https://wandb.ai/pabloufrn/rps_cnn_final</a><br/>\n","                Run page: <a href=\"https://wandb.ai/pabloufrn/rps_cnn_final/runs/27hlai3c\" target=\"_blank\">https://wandb.ai/pabloufrn/rps_cnn_final/runs/27hlai3c</a><br/>\n","                Run data is saved locally in <code>/content/wandb/run-20210427_025647-27hlai3c</code><br/><br/>\n","            "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"Bp7bqAvwg7mb"},"source":["## 3.2 Setup seeds"]},{"cell_type":"code","metadata":{"id":"-BT4gf8ShCoO"},"source":["from numpy.random import seed\n","seed(1)\n","from tensorflow.random import set_seed\n","set_seed(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CFwa9y74MEhy"},"source":["## 3.4 Obtaining the top spot on the Kaggle Leaderboard"]},{"cell_type":"markdown","metadata":{"id":"uF_lpAJ7Nm7s"},"source":["Of course, if you were to look at the Kaggle Dogs vs. Cats leaderboard, you would notice that to even break into the top-25 position we would need 96.98% accuracy, which our current method is not capable of reaching. So, what’s the solution?\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DcfAFunl6jY0"},"source":["### 3.4.1 Fine-tuning"]},{"cell_type":"code","metadata":{"id":"Lequt3Gn7G0B"},"source":["# import the necessary packages\n","from tensorflow.keras.applications import VGG19\n","from tensorflow.keras.applications import imagenet_utils\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.preprocessing.image import load_img\n","from sklearn.preprocessing import LabelEncoder\n","from imutils import paths\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import SGD\n","import numpy as np\n","import progressbar\n","import h5py\n","import random\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tQFxYXJ6RSrO"},"source":["# define the path to the output training, validation, and testing\n","# HDF5 files\n","TRAIN_HDF5 = \"RPS/hdf5/train.hdf5\"\n","VAL_HDF5 = \"RPS/hdf5/val.hdf5\"\n","TEST_HDF5 = \"RPS/hdf5/test.hdf5\"\n","\n","# path to the output model file\n","MODEL_PATH = \"RPS/vgg19_other_rps.model\"\n","\n","# define the path to the dataset mean\n","DATASET_MEAN = \"RPS/rps_mean.json\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xm87KJ537Ead"},"source":["# whether or not to include top of CNN\n","include_top = 0\n","\n","# load the ResNet50 network\n","print(\"[INFO] loading network...\")\n","model = VGG19(weights=\"imagenet\", include_top= include_top > 0)\n","print(\"[INFO] showing layers...\")\n","\n","# loop over the layers in the network and display them to the\n","# console\n","for (i, layer) in enumerate(model.layers):\n","\tprint(\"[INFO] {}\\t{}\".format(i, layer.__class__.__name__))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z5_xGqkF6lVJ"},"source":["# import the necessary packages\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dense\n","\n","# a fully connect network\n","class FCHeadNet:\n","\t@staticmethod\n","\tdef build(baseModel, classes, D):\n","\t\t# initialize the head model that will be placed on top of\n","\t\t# the base, then add a FC layer\n","\t\theadModel = baseModel.output\n","\t\theadModel = Flatten(name=\"flatten\")(headModel)\n","\t\theadModel = Dense(D, activation=\"relu\")(headModel)\n","\t\theadModel = Dropout(0.5)(headModel)\n","\n","\t\t# add a softmax layer\n","\t\theadModel = Dense(classes, activation=\"softmax\")(headModel)\n","\n","\t\t# return the model\n","\t\treturn headModel"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R6e9nblD6x7z"},"source":["%%wandb\n","# import the necessary packages\n","# set the matplotlib backend so figures can be saved in the background\n","import matplotlib\n","matplotlib.use(\"Agg\")\n","\n","# import the necessary packages\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.optimizers import Adam\n","from keras.callbacks import ModelCheckpoint\n","\n","import json\n","import os\n","\n","# Configurations related to checkpoint\n","# resume or not the model\n","resume = False\n","\n","\n","# construct the training image generator for data augmentation\n","aug = ImageDataGenerator(rotation_range=5, zoom_range=0.3,\n","                         width_shift_range=0.2,\n","                         height_shift_range=0.2,\n","                         shear_range=0.15,\n","                         horizontal_flip=False, fill_mode=\"nearest\")\n","\n","# load the RGB means for the training set\n","means = json.loads(open(DATASET_MEAN).read())\n","\n","# initialize the image preprocessors\n","sp = SimplePreprocessor(224, 224)\n","pp = PatchPreprocessor(224, 224)\n","mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\n","iap = ImageToArrayPreprocessor()\n","\n","# initialize the training and validation dataset generators\n","trainGen = HDF5DatasetGenerator(TRAIN_HDF5, config.batch_size, aug=aug,preprocessors=[pp, mp, iap], classes=3)\n","testGen = HDF5DatasetGenerator(TEST_HDF5, config.batch_size, preprocessors=[sp, mp, iap], classes=3)\n","valGen = HDF5DatasetGenerator(VAL_HDF5, config.batch_size, preprocessors=[sp, mp, iap], classes=3)\n","\n","\n","baseModel = VGG19(weights=\"imagenet\", include_top=False,input_tensor=Input(shape=(224, 224, 3)))\n","\n","# initialize the new head of the network, a set of FC layers\n","# followed by a softmax classifier\n","num_classes = 3\n","headModel = FCHeadNet.build(baseModel, num_classes, 256)\n","\n","# place the head FC model on top of the base model -- this will\n","# become the actual model we will train\n","model = Model(inputs=baseModel.input, outputs=headModel)\n","\n","# loop over all layers in the base model and freeze them so they\n","# will *not* be updated during the training process\n","for layer in baseModel.layers:\n","\tlayer.trainable = False\n","\n","# compile our model (this needs to be done after our setting our\n","# layers to being non-trainable\n","print(\"[INFO] compiling model...\")\n","\n","# RMSprop is frequently used in situations where we need to quickly obtain\n","# reasonable performance (as is the case when we are trying to “warm up” a set of FC layers).\n","opt = RMSprop(lr=config.learn_rate)\n","model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n","              metrics=[\"accuracy\"])\n","\n","# train the network\n","history = model.fit(trainGen.generator(),\n","          steps_per_epoch=(trainGen.numImages // config.batch_size) + 1,\n","          validation_data=testGen.generator(),\n","          validation_steps=(testGen.numImages // config.batch_size) + 1,\n","          epochs=config.epoch_mlp, callbacks= [WandbCallback(), ModelCheckpoint(MODEL_PATH)],\n","          max_queue_size=10,\n","          verbose=1)\n","# train the network\n","# save the model to file\n","print(\"[INFO] serializing model...\")\n","model.save(MODEL_PATH, overwrite=True)\n","\n","# close the HDF5 datasets\n","trainGen.close()\n","testGen.close()\n","valGen.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"whKOkPdzX7NH"},"source":["%%wandb\n","# initialize the training and validation dataset generators\n","trainGen = HDF5DatasetGenerator(TRAIN_HDF5, config.batch_size, aug=aug,preprocessors=[pp, mp, iap], classes=3)\n","testGen = HDF5DatasetGenerator(TEST_HDF5, config.batch_size, preprocessors=[sp, mp, iap], classes=3)\n","valGen = HDF5DatasetGenerator(VAL_HDF5, config.batch_size, preprocessors=[sp, mp, iap], classes=3)\n","\n","# now that the head FC layers have been trained/initialized, lets\n","# unfreeze the final set of CONV layers and make them trainable\n","for layer in baseModel.layers[-4:]:\n","\tlayer.trainable = True\n","\n","# for the changes to the model to take affect we need to recompile\n","# the model, this time using SGD with a *very* small learning rate\n","print(\"[INFO] re-compiling model...\")\n","opt = SGD(lr=config.learn_rate)\n","model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n","\tmetrics=[\"accuracy\"])\n","\n","# train the model again, this time fine-tuning *both* the final set\n","# of CONV layers along with our set of FC layers\n","print(\"[INFO] fine-tuning model...\")\n","# train the network\n","history = model.fit(trainGen.generator(),\n","          steps_per_epoch=(trainGen.numImages // config.batch_size) + 1,\n","          validation_data=testGen.generator(),\n","          validation_steps=(testGen.numImages // config.batch_size) + 1,\n","          epochs=config.epoch_fine, callbacks= [WandbCallback()],\n","          max_queue_size=10,\n","          verbose=1)\n","\n","# close the HDF5 datasets\n","trainGen.close()\n","testGen.close()\n","valGen.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KgfUyOMCW-Ay","executionInfo":{"status":"ok","timestamp":1619477141003,"user_tz":180,"elapsed":3174,"user":{"displayName":"Pablo Emanuell","photoUrl":"","userId":"14886274924574131634"}},"outputId":"1e500565-b765-4a8a-8ed4-62aa24b83ce9"},"source":["model.save(MODEL_PATH, overwrite=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Assets written to: RPS/vgg19_rps.model/assets\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j8rnvouYXK3n"},"source":["!cp -r /content/RPS/vgg19_rps.model /content/drive/MyDrive/data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SquztSi2Oq3G"},"source":["## Test Model"]},{"cell_type":"code","metadata":{"id":"YhbHP3yaO3gt"},"source":["!cp -r /content/drive/MyDrive/data/vgg_rps_final_98.modelcp/ /content/RPS/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xcq5ufPoPIoF"},"source":["import tensorflow as tf\n","model = tf.keras.models.load_model('/content/RPS/vgg_rps_final_98.modelcp')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O3tmEhWxs5CP"},"source":["# load the RGB means for the training set\n","means = json.loads(open(DATASET_MEAN).read())\n","\n","# initialize the image preprocessors\n","sp = SimplePreprocessor(224, 224)\n","pp = PatchPreprocessor(224, 224)\n","mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\n","iap = ImageToArrayPreprocessor()\n","\n","#\n","testGen = HDF5DatasetGenerator(TEST_HDF5, 128, preprocessors=[sp, mp, iap], classes=3)\n","valGen = HDF5DatasetGenerator(VAL_HDF5, 128, preprocessors=[sp, mp, iap], classes=3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VxdeXQsgZEFS","executionInfo":{"status":"ok","timestamp":1619536944837,"user_tz":180,"elapsed":12307,"user":{"displayName":"Pablo Emanuell","photoUrl":"","userId":"14886274924574131634"}},"outputId":"cfcb0e5b-acdb-4427-d265-dead3b465019"},"source":["from tensorflow import  argmax\n","\n","predctions = argmax(model.predict(valGen.generator(),\n","          steps=(valGen.numImages // 128) + 1,\n","          max_queue_size=10,\n","          verbose=1), 1)\n","labels =  argmax(next(valGen.generator())[1], 1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1/1 [==============================] - 11s 11s/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tmeHPvd4U5Wj","executionInfo":{"status":"ok","timestamp":1619494154464,"user_tz":180,"elapsed":5195,"user":{"displayName":"Pablo Emanuell","photoUrl":"","userId":"14886274924574131634"}},"outputId":"b82490eb-a14d-4d74-cd35-2f458b7d326e"},"source":["opt = SGD(lr=0.001)\n","model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n","              metrics=[\"accuracy\"])\n","model.evaluate(testGen.generator(),\n","          steps=(testGen.numImages // config.batch_size) + 1,\n","          max_queue_size=10,\n","          verbose=1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["3/3 [==============================] - 3s 1s/step - loss: 0.0652 - accuracy: 0.9695\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.12589725852012634, 0.9408602118492126]"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":475},"id":"9Us6y20hc172","executionInfo":{"status":"ok","timestamp":1619494205748,"user_tz":180,"elapsed":931,"user":{"displayName":"Pablo Emanuell","photoUrl":"","userId":"14886274924574131634"}},"outputId":"6dc6f359-fe66-42ac-bf80-9709a1564300"},"source":["from sklearn.metrics import confusion_matrix\n","import pandas as pd\n","import seaborn as sn\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import numpy as np\n","from tensorflow.math import confusion_matrix\n","cm = confusion_matrix(labels, predicitions, num_classes=3)\n","\n","y_true = labels.numpy()\n","y_pred = predicitions.numpy()\n","data = cm.numpy()\n","df_cm = pd.DataFrame(data, columns=np.unique(y_true), index = np.unique(y_true))\n","df_cm.index.name = 'Real'\n","df_cm.columns.name = 'Previsão'\n","fig = plt.figure(figsize = (10,7))\n","sn.set(font_scale=1.4)#for label size\n","sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})# font size"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7fa3a0143390>"]},"metadata":{"tags":[]},"execution_count":99},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAkcAAAG5CAYAAACEM5ADAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3debxVdbnH8c8BEUWEgzggKqiAjxipOJSa5VCEw9WuJmaD49XqljlkapnX9Fqa2i1zqGvdkkqTnMoh01Ihh9QySXP6paAMijgxiCAinPvH3kcXh8OwDhvW2Wt/3r3267DXXnuv59B5eb48z/qt1dTS0oIkSZIquhRdgCRJUmdiOJIkScowHEmSJGUYjiRJkjIMR5IkSRmGI0mSpIw1ii5gVVl73x94jQLVzIxbTi66BJXM63PeLroElUz/5jWbVufx1h5+fM1+z84bf9lqrX157BxJkiRllLZzJEmSVqGm8vZXDEeSJCm/pk41Caup8sY+SZKkDrBzJEmS8nOsJkmSlOFYTZIkqTHYOZIkSfk5VpMkScpwrCZJktQY7BxJkqT8HKtJkiRlOFaTJElqDHaOJElSfo7VJEmSMhyrSZIkNQY7R5IkKT/HapIkSRmO1SRJkhqDnSNJkpSfYzVJkqSMEoej8n5nkiRJHWDnSJIk5delvCdkG44kSVJ+jtUkSZIag50jSZKUX4mvc2Q4kiRJ+TlWkyRJagx2jiRJUn4FjdUiYjDwNWAXYBjwdEppWDv77Qt8B9gGeAG4OKV06Yocw86RJEnKr6lL7R75vA/YH3gWeLK9HSJiV+BmYDywL3AlcHFEfHFFDmDnSJIk5VfcCdm3pJRuAoiI0cBO7exzFvBISuk/qs/HRsQA4FsR8ZOU0qJlHcDOkSRJqhvLCzYR0R3YG/hNm5d+DfQDdljeMewcSZKk/Gq4Wi0imoHmdl6amVKamfPjBgFrsuTI7Ynq162Bh5f1AXaOJElSfk1NtXvAScBz7TxO6kBlfapf24aqGdWv6y3vA+wcSZKkol0MjG5ne96uUU0YjiRJUn41HKtVR2e1CkKtHaK2Y7rWjtLry/sAx2qSJCm/2o7VamkC8DYwtM32bapfn17eBxiOJElSaaSU5gN3A4e2eenTwEvAI8v7DMdqkiQpv4LurRYRPYD9qk8HAr0i4pDq87+llCYB/w3cExE/Ba4GPgQcB3x5eZcCAMORJEnqiOJuPLshcF2bba3PjwZGp5QeiIhPAOcBRwAvAienlP53RQ5gOJIkSXUjpfQ8sNwTlVJKtwG3deQYhiNJkpRfcbcPWeUMR5IkKb/ixmqrXHm/M0mSpA6wcyRJkvJzrCZJkpThWE2SJKkx2DmSJEn5OVaTJEl6T1OJw5FjNUmSpAw7R5IkKbcyd44MR5IkKb/yZiPHapIkSVl2jiRJUm6O1SRJkjIMR6pbm6zfk1NG7cQOQzbi/VtsQI+1uhFH/ozJL89ebL/u3bryrSN247C9h9K8Tncem/gy3/z5fdz/+AsFVa568tK0aVx0wfk8+MD9tLS08MFdd+O0089g4/79iy5NdeiV6S9xza9+TnrqCSY88y/mz3+La357O/36b1J0aWoQnnNUcltu3MzBH96KGXPmc/8TSw86/3vyCI7eZxjn/uovHHz273jp9Te55dsHs+2WG6zGalWP5s2bx3HHHMlzz03k3PMu4DvfvZDJkyZx7DFHMHfu3KLLUx16YeoUxt15B+uu24v3b79D0eVoKZqammr26GzsHJXcfY9PZfPP/ASAo0YOY8SOmy+xz/u3WJ/D9hrK579/B7/605MA3PvYVB654gj+6/BdGXXOzauzZNWZG6+/lqlTp3DTrbczYOBAAIZsFRy430iuv/Y3HHHU0QVXqHqz7fAdufH2PwPw+5tu4OGH/lJwRWpPZww1tWLnqORaWpa/z/67DOLtBQu5/p5/vbtt4aIWrvvzvxix40DW7NZ1FVaoejdu7N1su+127wYjgE033Yzth+/AuLF3FViZ6lWXLv5qUrEK7RxFxFBgX2BrYL3q5teBp4HbUkpPF1VbI9lmYF+enz6LefPfWWz7U5Neo3u3NRi0cTNPTX6toOrU2U149ln23PujS2wfNGgwf/rj7QVUJGm1KG/jqJhwFBFrAz8DPgW8DUwAZlRf3ho4HLgoIsYA/5FSequIOhtFn3XXYuac+Utsf/2Nyl/7euuutbpLUh2ZNWsWvXr1WmJ77969mT17djvvkFQGZR6rFdU5ugAYAXwOuCGl9Hb2xYhYEzgYuKS674mrvUJJktSQigpHhwEnp5Suae/FalgaExHdgP/BcLRKzXzjLQZsuO4S21s7Rq0dJKk9vXr3ardDtLSOkqRyKHPnqKiz3tYGpq/AftOr+2oVenLya2y+UW/W7r54Vt56wHrMX/AOE6bNLKgy1YNBgwYz4dlnltg+ceIEthw0uICKJK0OZV7KX1Q4uh84KyL6LG2H6mv/Bdy72qpqULc9NJE1u3Xl4A9v9e62rl2aOOQjW3HnI5N5e8HCAqtTZ7fnXnvzz8ceZeqUKe9ue+GFqfxj/CPssdfeBVYmSR1T1FjteGAcMDki7gKeBFrbE83AUOCj1W3+13UlHbT7EACGD9kQgJE7b86rs+bxyqy53PfPF3h0witc9+fERZ/fg25du/D89Fl8fv/t2Lxfb46+0NVGWraDDzmUMb++mhO/8iWOP+FEmmji8kt/yEb9+jFq1KeKLk916s93/RGAfz1dufbaQw/cR3NzH3r36cP2O+xcZGmq6owdn1ppalmRC+GsAhHRG/gilaX8Q4HWLtIM4CngNuCKlNKsjnz+2vv+oJhvrBOa94eT291+z2NTGHn69QCstWZXzjnyQxy659Y09+zOPye+wjd/fh/3/nPq6iy105pxS/t/h6qY9uKLi98+ZJddOfXrZ7DJJpsWXVqn9fqct5e/UwPb64Pvb3f7djvsxMU/vnI1V1Mf+jevuVrTSt8jr6nZ79nXfvHpTpW0CgtHq5rhSLVkOFKtGY5Ua4aj2vH2IZIkKbcyj9UMR5IkKbcyhyNvYCNJkpRh50iSJOVW5s6R4UiSJOVX3mzkWE2SJCnLzpEkScrNsZokSVJGmcORYzVJkqQMO0eSJCm3MneODEeSJCm3Mocjx2qSJEkZdo4kSVJ+5W0cGY4kSVJ+jtUkSZIahJ0jSZKUW5k7R4YjSZKUm+FIkiQpq7zZyHOOJEmSsuwcSZKk3ByrSZIkZZQ5HDlWkyRJyrBzJEmScitz58hwJEmScitzOHKsJkmSlGHnSJIk5VfexpHhSJIk5edYTZIkqUHYOZIkSbmVuXNkOJIkSbkVmY0i4t+BM4ChwJvA/cDXU0rP1OLzHatJkqS6EREfBW4EngYOBr4CbA3cGRG9anEMO0eSJCm3AsdqnwYmAUemlFoAImIS8BDwIeAPK3sAw5EkScqtwLFaN+CN1mBUNbP6tSZVGY4kSVI9GQ18JiK+AvwKaAa+BzwF3FWLAxiOJElSbrUcq0VEM5WQ09bMlNLM7IaU0tiIOBi4GrikuvlxYERKaX4t6vGEbEmSlFtTU+0ewEnAc+08Tmp73IjYDfgl8DNgb2AUsAi4OSLWrsX3ZudIkiQV7WIq47K2Zraz7RJgbErp5NYNEfEgMBk4HPjJyhZjOJIkSbl16VK7sdqTldFZe0GoPdsAN2c3pJSmRsSrwKBa1GM4kiRJuRW4Wm0SsGN2Q0QMBNYHnq/FAQxHkiSpnlwOXBoRlwI3AX2BM4GXgWtrcQDDkSRJyq3Ai0BeDrwNfAk4GngDeBA4NKX0Wi0OYDiSJEm5FZWNqhd//Ak1OPF6aVzKL0mSlGHnSJIk5VbgWG2VMxxJkqTcyhyOHKtJkiRl2DmSJEm5lbhxZDiSJEn5OVaTJElqEHaOJElSbiVuHBmOJElSfo7VJEmSGoSdI0mSlFuJG0eGI0mSlJ9jNUmSpAZh50iSJOVW4saR4UiSJOVX5rFaacPRjFtOLroElch237yj6BJUMlccs3PRJahk+jevV3QJpVHacCRJkladEjeODEeSJCm/Mo/VXK0mSZKUYedIkiTlVuLGkeFIkiTl51hNkiSpQdg5kiRJuZW4cWQ4kiRJ+TlWkyRJahB2jiRJUm5l7hwZjiRJUm4lzkaO1SRJkrLsHEmSpNwcq0mSJGWUOBsZjiRJUn5l7hx5zpEkSVKGnSNJkpRbiRtHhiNJkpRflxKnI8dqkiRJGXaOJElSbiVuHBmOJElSfq5WkyRJahB2jiRJUm5dyts4MhxJkqT8HKtJkiQ1CDtHkiQptxI3jgxHkiQpvybKm44cq0mSJGXYOZIkSbm5Wk2SJCnD1WqSJEkNws6RJEnKrcSNI8ORJEnKr0uJ05FjNUmSpAw7R5IkKbcSN44MR5IkKT9Xq0mSJDUIO0eSJCm3EjeODEeSJCk/V6tJkiQ1CDtHkiQpt6L7RhFxOHASsA0wF3gE+HRK6dWV/WzDkSRJyq3I1WoR8U3gG8D5wClAM7An0L0Wn284kiRJdSMiAjgbOCildGvmpd/V6hiGI0mSlFuX4hpHRwOT2gSjmjIcSZKk3Aocq+0CPBYRZwLHA32B8cCpKaU/1+IAhiNJklSoiGimct5QWzNTSjPbbOsH7AhsB5wAzAa+BtweEUNTSs+vbD0u5ZckSbk1NdXuQWXV2XPtPE5q59BdgJ7AJ1NK16aUbgcOpBKSTq3F92bnSJIk5VbjsdrFwOh2trftGgHMAF5LKf2jdUNKaW5EPAgMq0UxhiNJklSo6uisvSDUnieAQUt5ba1a1ONYTZIk5dalqXaPnG4F+kbEDq0bImIdYFfg77X43pbbOYqIszrwuS0ppXM78D5JklQHClyt9jvgr8D11YtBvkHlQpA9gO/X4gArMlY7uwOf2wIYjiRJUk2llBZFxP7A94AfURmlPQjsmVJ6thbHWG44Sik5epMkSYsp8t5q1funHbWqPt8TsiVJUm5dCry32qpmV0iSJCmjQ52jiBhG5aqUOwK9WTJktaSUlrbMTpIk1bkSN47yh6OI2B34EzAL+BswHLibyglRu1K5/kBNltJp1Xhp2jQuuuB8HnzgflpaWvjgrrtx2ulnsHH//kWXpjq1w8BmvvyxQQztvy5rdevK86/O5eq/TOaGh18oujTVoSceeZA7briKaVOeY+6cN+jZu5lBW7+ff/v0sfQfsEXR5amqwNVqq1xHOkfnApOADwLdgJeB81JKd0fEbsDvqdzjRJ3QvHnzOO6YI+m25pqce94FNDXBZZf8kGOPOYLrbryZHj16FF2i6kz068mVx+3Eo5Nn8V83PMG8BYsY+f6NOG/UMNZcowvXPDil6BJVZ958YzYDBgd77Hcw6/Zu5vVXpnP79b/iglOP5axLr6LvhhsXXaJKriPhaCfgv1NKsyJiveq2rgAppb9ExE+pBKg/1qhG1dCN11/L1KlTuOnW2xkwcCAAQ7YKDtxvJNdf+xuOOOrogitUvdlvu43p0tTEF0c/wty3FwLwl2deI/qtyyd26G84Um4f2OPjfGCPjy+2bfMh2/CtLx3GI/ePZcRBnymoMmWVuHHUoROyW3jvEt9vVr/2zbz+L2p0bxPV3rixd7Ptttu9G4wANt10M7YfvgPjxt5VYGWqV93WaOKdRYt4a8HCxbbPeeudjlz5VmpXz169AejStWvBlahVl6ammj06m46Eo+eALQFSSvOrz0dkXv8w8NrKl1YREQMi4ohafV6jm/DsswwastUS2wcNGszECTW5dpYazG8ffhGAMw8cyobrdmfdtdZg1Ac2ZZfB6zH6vkkFV6d6tmjhQt5ZsIDpL07hqssvoFefvuz8kRHLf6O0kjoyVvsjcCjwjerznwLnRcTmVK4JtSdwQS2Kq9oZuBL4ZQ0/s2HNmjWLXr16LbG9d+/ezJ49u4CKVO+emT6HI674G5cdMZzP7jYAgLffWcTZv32S2x59qeDqVM/OP/VYJj/7NAAbbrwpX/32ZfRqXm8579Lq0gkbPjXTkXB0HjAmIrqllBZQCUJdgUOAhVRuN3J+zSqU1KkN7NuDSw7fnmemz+FbNz7JWwsW8tH3bcjZB23D/AWLuOUf04ouUXXqmJO/xVtz3+SVl17gT7/7NRefdQKnfvcK1t/IE7I7A1erZaSUZpBZqp9SagG+U32ssIh4bAV3XbLNoQ7r1btXux2ipXWUpOX56j5DeGdhC1+88hHeWdQCwIMTXqdPj25888CtufXRabS0FFyk6tLGm20OwBbxPobtuCtnHHcwd9zwSz77pdOLLUylt1K3D4mIIcCGwOMppVk53z6UyjWRxi9nv4HAZh0oT+0YNGgwE559ZontEydOYMtBgwuoSPVuq349eXraG+8Go1aPTZnFAcP703edNXl1ztsFVaey6NFzXTbYeFNenja16FJUVeZbbHT0CtmfAb4LbFLdNAK4OyLWB/4CnJlSunY5H/M48ExKaZlrxyPik8AeHalTS9pzr735/vcuZOqUKWy6WSVzvvDCVP4x/hFOOPmUgqtTPXrljbcZuvG6dOvaxIKF7wWkbTdr5q0FC5k1b0GB1aksZs94nelTJy2xxF/FcayWUQ0rV1G5SvbFwPdaX0spvRoRTwFHAMsLRw8B+67gYcv7/8BqdvAhhzLm11dz4le+xPEnnEgTTVx+6Q/ZqF8/Ro36VNHlqQ5d/cBkLvnc9vz4qB245oHJvLVgEXtvsyEHDN+YK+99frHAJK2IH593OgO2DDbZfDBr91iH6S9O5s6bxtCla1dG/LvXONKq15HO0TeBO1NKIyOiL5lwVPUQ8J8r8DkXAbetwH63AV4vvkZ69OjBT3/+Cy664Hy++fXTKrcP2WVXTv36GfRYZ52iy1MduuOf0znu53/n2D224NufHEb3bl2Y/Npczvntk4x5yAtAKr8tYhh/v+8u/nTTNbyzYAHrbbARWw0bzj6HHOnJ2J1Ima9j1pFwNBT46jJefxnYYHkfklKaAExYgf3mUbldiWpk4/79+f4PLy26DJXIPelV7kmvFl2GSmKfTx7OPp88vOgytByGo8W9CfRcxuuDAP8rKUlSiZX5nKOOnGx+N3BURKzZ9oWI6A8cB9yxsoVJkiQVoaPnHD0EPAxcR+Vea/tFxMepBKNFwDk1q1CSJHU6ZR6r5e4cpZSeAT4EvETlathNVM5BOg34B7Ab4IUoJEkqsaam2j06mw5d5yil9BTw8YjoAwymErImArOAY4DbqZx7JEmSVFdWOBxVzzE6kEromQHcmlJ6EfhbRKwNfAU4CegHeHt3SZJKrEtnbPnUyAqFo+qJ1uOoBKPWv423IuIA4C3gGmBT4AHgy8Dval6pJEnqNLx9SOWmslsAFwL3Vv98FnAF0JfKrUA+nVK6f1UUKUmStLqsaDgaAVyZUvpG64aIeInKarVbgINSSotWQX2SJKkTKvFUbYXD0UbAg222tT4fbTCSJKmxlPmcoxUdGXalcm5RVuvzWbUrR5IkqVh5lvJvGREfyDzvXf26dUTMabtzSumvK1WZJEnqtErcOMoVjs6h/Stft72DaROVq2Z37WhRkiSpcyvzFbJXNBwdvUqrkCRJ6iRWKByllH6xqguRJEn1o8wnZHfo9iGSJKmxlTgblfoCl5IkSbnZOZIkSbl5QrYkSVJGE+VNR47VJEmSMuwcSZKk3ByrSZIkZZQ5HDlWkyRJyrBzJEmScmsq8YWODEeSJCk3x2qSJEkNws6RJEnKrcRTNcORJEnKr8w3nnWsJkmSlGHnSJIk5VbmE7INR5IkKbcST9Ucq0mSJGXZOZIkSbl1obytI8ORJEnKzbGaJElSg7BzJEmScnO1miRJUoYXgZQkSWoQdo4kSVJuJW4cGY4kSVJ+ZR6rGY4kSVJdioiewNPAJsDOKaWHa/G5hiNJkpRbJ2kcnc0qyDKekC1JknLrUsNHR0TEMOCLwFkd/iaWwnAkSZLq0eXAZcC/av3BjtUkSVJuTQXO1SLicGAwsD+wU60/33AkSZJyq2U0iohmoLmdl2amlGa22bc3cBFwSkppTkTUsJIKx2qSJKloJwHPtfM4qZ19vw08k1K6elUVY+dIkiTlVuPrHF0MjG5ne9uu0fuonIQ9otptAujZ+jUi1k0pvbGyxRiOJElSbrWMRtXR2czl7ghDqGSXse28NhZ4FNh+ZesxHEmSpHpxH7BXm23bAz+g0lH6ey0OYjiSJEm5FbFYLaX0KjAuuy1zQvbfvUK2JEkqTJFL+Vc1w5EkSapbKaVx1PYUKMORJEnKr8zXAjIcSZKk3ByrSZIkZZQ3GpW7KyZJkpSbnSNpBTz6nZFFl6CS6bPz8UWXoJKZN/6y1Xo8x2qSJEkZZR49lfl7kyRJys3OkSRJys2xmiRJUkZ5o5FjNUmSpMXYOZIkSbmVeKpmOJIkSfl1KfFgzbGaJElShp0jSZKUm2M1SZKkjCbHapIkSY3BzpEkScrNsZokSVKGq9UkSZIahJ0jSZKUm2M1SZKkjDKHI8dqkiRJGXaOJElSbmW+zpHhSJIk5dalvNnIsZokSVKWnSNJkpSbYzVJkqQMV6tJkiQ1CDtHkiQpN8dqkiRJGa5WkyRJahB2jiRJUm6O1SRJkjJcrSZJktQg7BxJkqTcStw4MhxJkqT8upR4ruZYTZIkKcPOkSRJyq28fSPDkSRJ6ogSpyPHapIkSRl2jiRJUm5eBFKSJCmjxIvVHKtJkiRl2TmSJEm5lbhxZDiSJEkdUOJ05FhNkiQpw86RJEnKzdVqkiRJGa5WkyRJahB2jiRJUm4lbhwZjiRJUgeUOB05VpMkScqwcyRJknJztZokSVKGq9UkSZIahJ0jSZKUW4kbR4YjSZLUASVOR4YjSZKUW1EnZEfEKOCzwI7AesAE4MfAFSmlRbU4huFIkiTVk1OAScCpwHRgL+ASYMvqtpVmOJIkSbkVuFrtgJTSK5nnYyOiJ3B8RJyZUpq/sgdwtZokScqtqYaPPNoEo1bjgbWojNlWmp2jBvTStGlcdMH5PPjA/bS0tPDBXXfjtNPPYOP+/YsuTXXKnyl11CYbNnPK0SPYYZsBvH/IJvRYe01iv7OYPO31xfY75/gD2GGbAQwfOoC+zetw3Fm/4qpbHiqoatVaRDQDze28NDOlNHMFPuLDwOvAy7Wox85Rg5k3bx7HHXMkzz03kXPPu4DvfPdCJk+axLHHHMHcuXOLLk91yJ8prYwtN9uAg0cMZ8bsudw/fsJS9/vPw/Zg7e7d+MO9j6/G6rRMtW0dnQQ8187jpOWVERE7AUcDP0gpLazFt2bnqMHceP21TJ06hZtuvZ0BAwcCMGSr4MD9RnL9tb/hiKOOLrhC1Rt/prQy7nvkWTb/2BkAHHXQrozYbWi7+2304VNpaWlhy83W53MHfHB1lqilqPFqtYuB0e1sX2bXKCL6ATcAfwUuqFUxhqMGM27s3Wy77Xbv/hID2HTTzdh++A6MG3uXv8iUmz9TWhktLS013U/1qTo6W5Hx2bsiojfwB2AucGBKaUGt6il0rBYR3SJio4hoN35GxLoR8ZHVXVeZTXj2WQYN2WqJ7YMGDWbihGcLqEj1zp8pqTE1NdXukVdErAXcDGwI7JNSeq2W31sh4SgimiLiAiop8UXg5Yj4RkR0bbPrNsDY1V5gic2aNYtevXotsb13797Mnj27gIpU7/yZkhpTUavVImIN4FpgW2DflNKklf1e2ipqrPYF4GTgUirL7z4CnAPsGxGfSCnNKKguSZLUuV0OHACcBvSIiF0yrz2ZUlrpf5UVFY7+EzgvpXR29flVEfETKidV3RsR+6SUphZUW6n16t2r3X/NL+1f/9Ly+DMlNajiLgI5svr1wnZe2wsYt7IHKCocDaLNuCyl9HBEfJDKyVUPRMQ+hVRWcoMGDWbCs88ssX3ixAlsOWhwARWp3vkzJTWmou6tllLafFUfo6gTsl8HNmq7MaX0ErAHMBG4B9h9NddVenvutTf/fOxRpk6Z8u62F16Yyj/GP8Iee+1dYGWqV/5MSSqbojpHfwcOonJC1WJSSrMj4uPAdcBFgOs3a+jgQw5lzK+v5sSvfInjTziRJpq4/NIfslG/fowa9amiy1Md8mdKK+ugj20PwPChAwAYufs2vDpjDq/MmMN9f6+seNx9x8Fs0KcnG/WtjGp33GYAb86r3ELrt3f+o4CqVeC91Va5piKuHRERo4CvAv+2tOV31ZVrPwI+nlLaIu8x3nrHULU00158cfFbPeyyK6d+/Qw22WTToktTnfJnKr8+Ox9fdAmdxrzxl7W7/Z6Hn2HkcT8E4I6fnshHdhrS7n5rD/fvEmDe+MtWa1z510tza/Z7dqt+PTpV1CokHK0OhiNJnZnhSLVmOKodr5AtSZLy61RxprYMR5IkKbeiVqutDoXePkSSJKmzsXMkSZJyK/NqNcORJEnKrcTZyLGaJElSlp0jSZKUX4lbR4YjSZKUm6vVJEmSGoSdI0mSlJur1SRJkjJKnI0cq0mSJGXZOZIkSfmVuHVkOJIkSbm5Wk2SJKlB2DmSJEm5uVpNkiQpo8TZyLGaJElSlp0jSZKUm2M1SZKkxZQ3HTlWkyRJyrBzJEmScnOsJkmSlFHibORYTZIkKcvOkSRJys2xmiRJUob3VpMkSWoQdo4kSVJ+5W0cGY4kSVJ+Jc5GjtUkSZKy7BxJkqTcXK0mSZKU4Wo1SZKkBmHnSJIk5VfexpHhSJIk5VfibORYTZIkKcvOkSRJys3VapIkSRllXq1mOJIkSbmVuXPkOUeSJEkZhiNJkqQMx2qSJCk3x2qSJEkNws6RJEnKzdVqkiRJGY7VJEmSGoSdI0mSlFuJG0eGI0mS1AElTkeO1SRJkjLsHEmSpNxcrSZJkpThajVJkqQGYedIkiTlVmTjKCKGAJcCuwPzgDHA6SmlubX4fMORJEnKr6B0FBHNwFhgEnAIsCHwfWAD4LBaHMNwJEmS6skXgD7A9imlVwEi4h3g6og4N6X0xMoewHOOJElSbk01/F9O+wF3tQajqhuA+cC+tfje7BxJkqTcarlarToqa27npZkppZlttg0Ffp7dkFKaHxETgK1rUU9pw9Faa5T4AgyS6sZRnmgAAAfXSURBVN688ZcVXYK0Umr8e/Zs4FvtbD+n+lpWH6BtYAKYAaxXi2JKG44kSVLduBgY3c729kLQKmc4kiRJhaqOzlY0CM2g/RFcH+DpWtTjCdmSJKmePEXlvKN3RUR3YBCGI0mS1IBuAz4aEX0z2w4CuldfW2lNLS0ttfgcSZKkVa66su1x4HngXN67CORdKaWaXATSzpEkSaob1fOT9gbmADcCPwB+AxxTq2PYOZIkScqwcyRJkpRhOJIkScrwOkcNKCKGAJcCuwPzgDHA6SmluYUWproVEYOBrwG7AMOAp1NKw4qtSvUqIkYBnwV2pHLF4wnAj4ErUkqLiqxNjcFw1GCqZ/mPBSYBh/DeWf4bADU5y18N6X3A/sBDVDrSdqW1Mk6h8t+oU4HpwF7AJcCW1W3SKmU4ajxfoHIV0e1b72gcEe8AV0fEuSmlJwqtTvXqlpTSTQARMRrYqdhyVOcOSCm9knk+NiJ6AsdHxJkppflFFabG4L/uGs9+VK4F8Wpm2w3AfGDfYkpSvXPUoVpqE4xajQfWokY3FpWWxXDUeIYCT2Y3VP8VNgHYupCKJGn5Pgy8DrxcdCEqP8NR4+lD+zf3m4H/IpPUCUXETsDRwA9SSguLrkflZziSJHVaEdGPyuj/r8AFBZejBmE4ajwzgOZ2tveh0rKWpE4hInoDfwDmAgemlBYUXJIahOGo8TxF5byjd0VEd2AQ8HQhFUlSGxGxFnAzlcuN7JNSeq3gktRADEeN5zbgoxHRN7PtIKB79TVJKlRErAFcC2wL7JtSmlRwSWow3ni2wVQvAvk48DxwLu9dBPKulJIXgVSHREQPKpeJAPgylU7kV6vP/+YvN+UREVcAnwdOA+5t8/KTKaXZq78qNRIvAtlgUkozI2JvKlebvZH3bh9yWqGFqd5tCFzXZlvr86OB0au1GtW7kdWvF7bz2l7AuNVXihqRnSNJkqQMzzmSJEnKMBxJkiRlGI4kSZIyDEeSJEkZhiNJkqQMw5EkSVKG1zmStEpExDiAlNKeOd/3ISrX4HoZ+A/gY8CGKaWTalyiJLXLcCSVREQcBVyZ2bQQeAn4E3BmSumFIurqgBOB31Op/z5gAZUL/0nSamE4ksrnbGACsBbwIeAIYI+IGJZSmrsa6/h4B993EjAjpTQvIk4D3kkpvVHDuiRpmQxHUvnckVJ6sPrn/4uI16nc5+wTwDVtd46IdVJKb9a6iJTS2x1834uZP8+oXUWStGIMR1L53U0lHG0REaOBw4Ctqdxfb0/gkepXIuIzwMnAMOAt4E7gtJTSc9XXLwOOoXIO0JzsQSLi/4BPV197s71zjiLiUOBUIIAmYCrw65TSudXX1we+TqXrtEX1bQ8DZ6WUFrsBafVmt+cAnwI2AiYDPwMuTCkt6thflSS5Wk1qBIOqX1+rfu0C/BGYTSWo/AogIr4OXAU8B5wCfA/YHbg/IjaovncMsDZwYPYAEdENOAi4ZWldqIj4WPX9s4BvVI99e/UYrQZXP+c24GvAd4BNgTsjYtvMZzUBv6vu8ycqge4x4HzgRyv0tyJJS2HnSCqf3tUOTOs5R2cB84BbgV2BbsCtKaWvtr4hIgYA5wJnp5T+O7N9DPAElfBxBnA/MIVKt+bXmWOOANajEn6WZn/gDWBkSmnhUvYZDwzJdn4i4grgaeAE4Njq5gOqxzw7pXROdduPIuJK4AsRcVlK6fFl1CJJS2XnSCqf24FXqISYMcB04IA2q9XadlcOpvKPpd9ExPqtDypdnn9SXS2WUmoBrgVGRkTvzPs/Vd33D8uoaxawDss4UTulNL81GEXEWhHRF+gK/A3YMbPr/sAi4IdtPuJ/Mq9LUofYOZLK5wTgKSrnDE0GplRDTatFwPNt3rNV9evTS/nMiZk/j6EydjsIGB0R3YF/B36bUpq/jLp+BIwCbouIF6mcz3QDlVFcC0BEdAFOAz7Pe+cctXou8+eBwPSU0sw2+6Tq97f5MuqQpGUyHEnl87fMarX2LEgpvdNmW2sXeV+g7WtQGcsBkFJ6OCKepdItGl19Ty+WPVIjpfRyRAynclHHfYF9qFxm4NaIOLAakL4BfBv4BXAmlfOkFla3D2r3gyWpxgxHkqByXSSAySmlJ1dg/98Ap1fHXp8CXqXSCVqm6vL+26h0j5qonEB9OrAblfOZRgHjUkpHZd8XEee0+ahJwIiI6J1SmpXZvhWVoPf8CnwPktQuzzmSBJXx1kLgrGpoWUz1/KOsMVT+cfU5KidH39BON6rtZ/TNPq92isZXnzZXvy6kssQ/+77dqJxInnUrlf9+ndBme+tJ5r9fVi2StCx2jiSRUppYXcp/ETAwIn4HzKRy3s8nqHSKzs7s/3hEPEFlhds6LGekVvV/1ZB1F5WTxTcBjgemAfdU97kZODsifgncCwyhcv7Rk0DPzGfdSmUJ/zkRMZDKtZr2Bj4JXOFKNUkrw86RJABSSt+jcmL121TO9/k+lZOuxwHXtfOWMcC6wIu8F26W5SpgDvBFKidnH0tlxPahzO1BzgcupBJ0LqGySu4wKheCzNbaUq3t+1TOXboY2J7K5Qa+vAK1SNJSNbW0tCx/L0mSpAZh50iSJCnDcCRJkpRhOJIkScowHEmSJGUYjiRJkjIMR5IkSRmGI0mSpAzDkSRJUobhSJIkKcNwJEmSlPH/PgopIMyWhJwAAAAASUVORK5CYII=\n","text/plain":["<Figure size 720x504 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_PSpR8IOzFgI","executionInfo":{"status":"ok","timestamp":1619534866342,"user_tz":180,"elapsed":4893,"user":{"displayName":"Pablo Emanuell","photoUrl":"","userId":"14886274924574131634"}},"outputId":"e5597374-dec3-40e6-82ba-172cb097f69d"},"source":["# download rock, paper, scissors dataset\n","!gdown --id 1ZrcUuGjgYlnu9CQnYtw6N0sshl7esfKd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1ZrcUuGjgYlnu9CQnYtw6N0sshl7esfKd\n","To: /content/rps.zip\n","237MB [00:02, 99.8MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JXql10eEzFgL"},"source":["!unzip rps.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IFxZve9vwy95"},"source":["IMAGES_PATH = \"RPS\"\n","trainPaths = list(paths.list_images(IMAGES_PATH+\"/train/\"))\n","testPaths = list(paths.list_images(IMAGES_PATH+\"/test/\")) \n","valPaths = list(paths.list_images(IMAGES_PATH+\"/validation/\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dxb0xmvPx3um"},"source":["raw_img_list = []\n","for path in valPaths:\n","    print('')\n","    img = cv2.imread(path)\n","    raw_img_list.append(img)\n","raw_img_list = np.array(raw_img_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qAJaI4AltYqt"},"source":["wrong = np.where((predctions != labels).numpy())\n","image_list = next(valGen.generator())[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YlgYDju5t11P"},"source":["wrong_images = image_list[wrong[0]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H9Lelaoh6pFs","executionInfo":{"status":"ok","timestamp":1619536817613,"user_tz":180,"elapsed":877,"user":{"displayName":"Pablo Emanuell","photoUrl":"","userId":"14886274924574131634"}},"outputId":"68ba6929-cf6b-4d41-d6c5-dbd00f8c3b00"},"source":["type(labels)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensorflow.python.framework.ops.EagerTensor"]},"metadata":{"tags":[]},"execution_count":133}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":234},"id":"drDyyTJ-gzj7","executionInfo":{"status":"ok","timestamp":1619537490658,"user_tz":180,"elapsed":768,"user":{"displayName":"Pablo Emanuell","photoUrl":"","userId":"14886274924574131634"}},"outputId":"8e8d69cd-d863-46dc-a117-a769b5da84c3"},"source":["fig, ax = plt.subplots(1,raw_wrong_images.shape[0],figsize=(10,10))\n","names = ['Paper', 'Rock', 'Scissors']\n","for idx, image in enumerate(wrong_images):\n","    ax[idx].imshow((image))\n","    ax[idx].axis('off')\n","    ax[idx].text(10, 10, f'Predicted: {names[predctions.numpy()[wrong[0][idx]]]}'+ \n","                f'\\nReal: {names[labels.numpy()[wrong[0][idx]]]}')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x720 with 4 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UXPSLnsD9UfE","executionInfo":{"status":"ok","timestamp":1619537576711,"user_tz":180,"elapsed":662,"user":{"displayName":"Pablo Emanuell","photoUrl":"","userId":"14886274924574131634"}},"outputId":"ac25e376-52d0-49cd-ac5f-0a921e2f92b2"},"source":["predctions.numpy()[wrong]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 2, 2, 2])"]},"metadata":{"tags":[]},"execution_count":169}]}]}